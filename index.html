<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model">
  <meta name="keywords" content="DrivingDiffusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/ms_icon.png">

  <style>  
    table {  
      font-family: arial, sans-serif;  
      border-collapse: collapse;  
      width: 100%;  
    }  
      
    td, th {  
      border: 2px solid #F1F4F5;  
      text-align: left;  
      padding: 8px;  
    }  
    tr:nth-child(3n - 1) {  
      background-color: #F1F4F5;  
    }  

    tr:nth-child(3n) {  
      border: 2px solid #FFFFFF;
    }  
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


	
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><strong>DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model </strong> </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Xiaofan Li , </span>
            <span class="author-block">
              Yifu Zhang , </span>
            <span class="author-block">
              Xiaoqing Ye </span>
            </span><br>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Baidu Inc. </span>&nbsp;
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/xxx.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/shalfun/DrivingDiffuison"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img id="teaser" autoplay muted loop playsinline height="100%" src="./static/images/intro_s.png" style="width:100%;height:100%;">
      <p  style="font-size: 16px;"> 
	      Examples of the generated multi-view video frames from a man-made 3D layout (including the 3D bounding boxes of obstacles and the road structure) by DrivingDiffusion. The 3D layout is projected to the six camera views using camera parameters. We present the generated results of six camera views and six consecutive frames.
      </p>
    </div>
  </div>
</section>



	
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
		  With the increasing popularity of autonomous driving based on the powerful and unified bird's-eye-view (BEV) representation, a demand for high-quality and large-scale multi-view video data with accurate annotation is urgently required. However, such large-scale multi-view data is hard to obtain due to expensive collection and annotation costs. To alleviate the problem, we propose a spatial-temporal consistent diffusion framework DrivingDiffusion, to generate realistic multi-view videos controlled by 3D layout. There are three challenges when synthesizing multi-view videos given a 3D layout: How to keep 1) cross-view consistency and 2) cross-frame consistency? 3) How to guarantee the quality of the generated instances? Our DrivingDiffusion solves the problem by cascading the multi-view single-frame image generation step, the single-view video generation step shared by multiple cameras, and post-processing that can handle long video generation. In the multi-view model, the consistency of multi-view images is ensured by information exchange between adjacent cameras. In the temporal model, we mainly query the information that needs attention in subsequent frame generation from the multi-view images of the first frame.  We also introduce the local prompt to effectively improve the quality of generated instances. In post-processing, we further enhance the cross-view consistency of subsequent frames and extend the video length by employing temporal sliding window algorithm. Without any extra cost, our model can generate large-scale realistic multi-camera driving videos in complex urban scenes, fueling the downstream driving tasks. The code will be made publicly available.
          </p>
        </div> 
      </div>
    </div>
    <!--/ Abstract. -->


  </div>
</section>


<section class="section" id="Method">

  <div class="container is-max-desktop content">
    <h2 class="title">Method</h2>
    <section class="hero method">
      <div class="container is-max-desktop">
        <div class="hero-body">
	<hr> <!-- 分隔线 --> 
	    <h3 class="title">[DrivingDiffusion] Training Pipeline</h2>

          <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/main.png" style="width:100%;height:100%;">
          <p>
		Diagram of the multi-view video generation framework DrivingDiffusion. For training, we separately train the multi-view model and the temporal model. These two models share similar structures, with the exception of the orange and purple components. During the inference stage, the two models are concatenated in a cascaded manner. First, the multi-view model generates the initial multi-view frame of the video. This frame is then set as the keyframe for the temporal model. Lastly, the temporal model generates video frames for each view, forming the final multi-view video.          </p>
        </div>
      </div>
    </section>
  </div>
</section>


<!-- 	inference fig -->
<hr> <!-- 分隔线 --> 
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img id="teaser" autoplay muted loop playsinline height="100%" src="./static/images/inference.png" style="width:100%;height:100%;">
      <p  style="font-size: 16px;"> 
	      Multi-view Long video generation pipeline. We introduce a multi-stage inference strategy to generate multi-view long videos: 1) We first adopt the multi-view model to generate the first frame panoramic image of the video sequence. 2) Then we use the generated image from each perspective as input for the temporal model, allowing for parallel sequence generation for each corresponding viewpoint. 3) For subsequent frames, we employ the finetune model for parallel inference as well. 4) Extend the video after identifying new keyframes, just like the sliding window algorithm does. Finally we obtain the entire synthetic multi-view video. 
      </p>
    </div>
  </div>
</section>

<!-- 	future pipe fig -->
<hr> <!-- 分隔线 --> 

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img id="teaser" autoplay muted loop playsinline height="100%" src="./static/images/future_pipe2.png" style="width:90%;height:90%;">
      <p  style="font-size: 16px;"> 
	      Diagram of future generation pipeline. We only input the first frame and predict the following frames. We believe that compared to the redundant information in the images, BEV layout is an intermediate representation that is easier for the model to learn the main elements of road conditions. We support both unconditional and text-controlled prediction of future scenes. For the text controller, we decouple the behavior of the ego-vehicle and other vehicles. (Omitting the concept branch and relying solely on the visual branch for future prediction can still yield satisfactory results in the short term.)
      </p>
    </div>
  </div>
</section>

	
<section class="section" id="Results">
  <div class="container is-max-desktop content">
    <h2 class="title">Results</h2>
    <section class="hero method">
    <div class="container is-max-desktop">
    <div class="hero-body">  


<!-- 	control fig -->
	    
<h4 class="title">1. Visualization of control capability of DrivingDiffusion </h4>
<hr> <!-- 分隔线 --> 

	<section class="hero teaser">
	  <div class="container is-max-desktop">
	    <div class="hero-body">
	      <div style="display: flex; justify-content: center; align-items: center;">  
	          <img id="control" autoplay muted loop playsinline height="100%" src="./static/images/control.png" style="width:100%;height:100%;">
		      <p  style="font-size: 16px;"> 
		      </p>
	      </div>
	    </div>
	  </div>
	</section>
	    

	    
<h4 class="title">2. Multi-View Video Generation of Driving Scenes Controlled by 3D Layout on nuScenes </h4>
<hr> <!-- 分隔线 --> 
	    
<section class="hero teaser">

	<style>  
	.video-container {  
	  display: grid;  
	  grid-template-columns: repeat(2, 1fr);  
	  grid-template-rows: repeat(2, 1fr);  
	  gap: 10px; /* 视频间的间距 */  
	}  
	.video-container video {  
	  width: 100%; /* 视频的宽度 */  
	  height: 100%; /* 视频的高度 */  
	}  
	</style>  
 
  
	<div class="video-container">  
	  <video controls>  
	    <source src="./static/videos/1.mp4" type="video/mp4">  
	    Ha.  
	  </video>  
	  <video controls>  
	    <source src="./static/videos/2.mp4" type="video/mp4">  
	    Ha.  
	  </video>  
	  <video controls>  
	    <source src="./static/videos/3.mp4" type="video/mp4">  
	    Ha.  
	  </video>  
	  <video controls>  
	    <source src="./static/videos/4.mp4" type="video/mp4">  
	    Ha.  
	  </video>  
	</div>  
<p style="margin-bottom: 30px;"></p>
</section>


	    
<h4 class="title">3. Multi-View Video Generation of Driving Scenes Controlled by 3D Layout on private dataset </h4>
<hr> <!-- 分隔线 --> 
	    
<section class="hero teaser">
	<style>  
	.video-container {  
	  display: grid;  
	  grid-template-columns: repeat(2, 1fr);  
	  grid-template-rows: repeat(1, 1fr);  
	  gap: 10px; /* 视频间的间距 */  
	}  
	.video-container video {  
	  width: 100%; /* 视频的宽度 */  
	  height: 100%; /* 视频的高度 */  
	}  
	</style>  
 
  
<div class="video-container">  
  <video controls>  
    <source src="./static/videos/private_1.mp4" type="video/mp4">  
    Ha.  
  </video>  
  <video controls>  
    <source src="./static/videos/private_1.mp4" type="video/mp4">  
    Ha.  
  </video>  
<br style="clear:both; height:10px;">

</div>  
</section>

<br style="clear:both; height:10px;">



<!--future fig -->
<h4 class="title">4. The ability to construct the future. </h4>
<hr> <!-- 分隔线 --> 

	    
Control future video generation through text description of road conditions.
<br style="clear:both; height:10px;">

	<section class="hero teaser">
	  <div class="container is-max-desktop">
	    <div class="hero-body">
	      <div style="display: flex; justify-content: center; align-items: center;">  
	          <img id="future" autoplay muted loop playsinline height="100%" src="./static/images/future.png" style="width:100%;height:100%;">
		      <p  style="font-size: 16px;"> 
		      </p>
	      </div>
	    </div>
	  </div>
	</section>

 Future video generation without text description of road conditions. We found this to be an excellent way to get a pre-trained diffusion model.
<br style="clear:both; height:10px;">

	<section class="hero teaser">
	  <div class="container is-max-desktop">
	    <div class="hero-body">
	      <div style="display: flex; justify-content: center; align-items: center;">  
	          <img id="future" autoplay muted loop playsinline height="100%" src="./static/images/future_unc.png" style="width:100%;height:100%;">
		      <p  style="font-size: 16px;"> 
		      </p>
	      </div>
	    </div>
	  </div>
	</section>
	    
	    
<!--     <div class="container is-max-desktop">
        <video preload="auto"poster="" id="tree" autoplay controls muted loop width="1000px" outline="0px"> 
          <source src="./static/videos/1.mp4"
          type="video/mp4">
        </video>
    </div> 
    <br>
    <p style="margin-bottom: 30px;"></p> -->







	
	
<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <p> If you use our work in your research, please cite: </p>
    <pre><code>@article{wang2023drive,
  title={DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving},
  author={Wang, Xiaofeng and Zhu, Zheng and Huang, Guan and Chen, Xinze and Lu, Jiwen},
  journal={arXiv preprint arXiv:2309.09777},
  year={2023}
}</code></pre>
  </div>
</section>
 -->


	
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/shalfun/DrivingDiffuison" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
