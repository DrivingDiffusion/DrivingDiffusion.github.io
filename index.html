<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model">
  <meta name="keywords" content="DrivingDiffusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/ms_icon.png">

  <style>  
    table {  
      font-family: arial, sans-serif;  
      border-collapse: collapse;  
      width: 100%;  
    }  
      
    td, th {  
      border: 2px solid #F1F4F5;  
      text-align: left;  
      padding: 8px;  
    }  
    tr:nth-child(3n - 1) {  
      background-color: #F1F4F5;  
    }  

    tr:nth-child(3n) {  
      border: 2px solid #FFFFFF;
    }  
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


	
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><strong>DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model </strong> </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Xiaofan Li , </span>
            <span class="author-block">
              Yifu Zhang , </span>
            <span class="author-block">
              Xiaoqing Ye </span>
            </span><br>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Baidu Inc. </span>&nbsp;
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/xxx.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/shalfun/DrivingDiffuison"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img id="teaser" autoplay muted loop playsinline height="100%" src="./static/images/intro.png" style="width:100%;height:100%;">
      <p  style="font-size: 16px;"> 
	      Examples of the generated multi-view video frames from a man-made 3D layout (including the 3D bounding boxes of obstacles and the road structure) by DrivingDiffusion. The 3D layout is projected to the six camera views using camera parameters. We present the generated results of six camera views and six consecutive frames.
      </p>
    </div>
  </div>
</section>



	
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
		  With the increasing popularity of autonomous driving based on the powerful and unified bird's-eye-view (BEV) representation, a demand for high-quality and large-scale multi-view video data with accurate annotation is urgently required. However, such large-scale multi-view data is hard to obtain due to expensive collection and annotation costs. To alleviate the problem, we propose a spatial-temporal consistent diffusion framework DrivingDiffusion, to generate realistic multi-view videos controlled by 3D layout. There are three challenges when synthesizing multi-view videos given a 3D layout: How to keep 1) cross-view consistency and 2) cross-frame consistency? 3) How to guarantee the quality of the generated instances? Our DrivingDiffusion solves the problem by cascading the multi-view single-frame image generation step, the single-view video generation step shared by multiple cameras, and post-processing that can handle long video generation. In the multi-view model, the consistency of multi-view images is ensured by information exchange between adjacent cameras. In the temporal model, we mainly query the information that needs attention in subsequent frame generation from the multi-view images of the first frame.Â  We also introduce the local prompt to effectively improve the quality of generated instances. In post-processing, we further enhance the cross-view consistency of subsequent frames and extend the video length by employing temporal sliding window algorithm. Without any extra cost, our model can generate large-scale realistic multi-camera driving videos in complex urban scenes, fueling the downstream driving tasks. The code will be made publicly available.
          </p>
        </div> 
      </div>
    </div>
    <!--/ Abstract. -->


  </div>
</section>


<section class="section" id="Method">
  <div class="container is-max-desktop content">
    <h2 class="title">Method</h2>
    <section class="hero method">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/main.png" style="width:100%;height:100%;">
          <p>
Diagram of the multi-view video generation framework DrivingDiffusion. For training, we separately train the multi-view model and the temporal model. These two models share similar structures, with the exception of the orange and purple components. During the inference stage, the two models are concatenated in a cascaded manner. First, the multi-view model generates the initial multi-view frame of the video. This frame is then set as the keyframe for the temporal model. Lastly, the temporal model generates video frames for each view, forming the final multi-view video.          </p>
        </div>
      </div>
    </section>
  </div>
</section>


<!-- 	inference fig -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img id="teaser" autoplay muted loop playsinline height="100%" src="./static/images/inference.png" style="width:100%;height:100%;">
      <p  style="font-size: 16px;"> 
	      Multi-view Long video generation pipeline.
      </p>
    </div>
  </div>
</section>


<section class="section" id="Results">
  <div class="container is-max-desktop content">
    <h2 class="title">Results</h2>
    <section class="hero method">
    <div class="container is-max-desktop">
    <div class="hero-body">  


<!-- 	control fig -->
	    
<h4 class="title">1. Visualization of control capability of Driving diffusion </h4>
	<section class="hero teaser">
	  <div class="container is-max-desktop">
	    <div class="hero-body">
	        <img id="teaser" autoplay muted loop playsinline height="100%" src="./static/images/control.png" style="width:100%;height:100%;">
	      <p  style="font-size: 16px;"> 
		      Multi-view Long video generation pipeline.
	      </p>
	    </div>
	  </div>
	</section>


	    
  <h4 class="title">2. Multi-view video generation of driving scene controlled by 3Dlayout </h4>
    <div class="container is-max-desktop">
        <video preload="auto"poster="" id="tree" autoplay controls muted loop width="1000px" outline="0px"> 
          <source src="./static/videos/1.mp4"
          type="video/mp4">
        </video>
    </div> 
    <br>
    <p style="margin-bottom: 30px;"></p>

	<h4 class="title">3. Multi-view video generation of driving scene controlled by 3Dlayout </h4>
  <div class="container is-max-desktop">
    <div style="text-align: center;">
      <video preload="auto" poster="" id="tree" autoplay controls muted loop width="700px" outline="0px" style="display: inline-block;">
        <source src="./static/videos/2.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  <br>
	<p style="margin-bottom: 30px;"></p>

  <h4 class="title">4. Multi-view video generation of driving scene controlled by 3Dlayout </h4>

  <div class="container is-max-desktop">
    <div style="text-align: center;">
      <video preload="auto" poster="" id="tree" autoplay controls muted loop width="700px" outline="0px" style="display: inline-block;">
        <source src="./static/videos/3.mp4" type="video/mp4">
      </video>
    </div>
  </div>
  <br>
	<p style="margin-bottom: 30px;"></p>

	<h4 class="title">5. Future Driving Action Generation.</h4>

  <div class="container is-max-desktop">
    <div style="text-align: center;">
      <video preload="auto" poster="" id="tree" autoplay controls muted loop width="700px" outline="0px" style="display: inline-block;">
        <source src="./static/videos/4.mp4" type="video/mp4">
      </video>
    </div>
  </div>

  </div></div></section>
  </div>
</section>

	
	
<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <p> If you use our work in your research, please cite: </p>
    <pre><code>@article{wang2023drive,
  title={DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving},
  author={Wang, Xiaofeng and Zhu, Zheng and Huang, Guan and Chen, Xinze and Lu, Jiwen},
  journal={arXiv preprint arXiv:2309.09777},
  year={2023}
}</code></pre>
  </div>
</section>
 -->


	
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/shalfun/DrivingDiffuison" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
